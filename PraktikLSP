import face_recognition 
import cv2
import numpy as np
from gpiozero import LED, Button 
from time import sleep

led = LED(26)
alarm = LED(19) 
tombol = Button(6) 
sensor = Button(21)
tombol_dalam = Button(13)


keamanan = 1
i = 0


video_capture = cv2.VideoCapture(0)


# Load a sample picture and learn how to recognize it. 
user_1_image = face_recognition.load_image_file("user1.JPG")
user_1_face_encoding = face_recognition.face_encodings(user_1_image)[0]


# Load a second sample picture and learn how to recognize it. 
user_2_image = face_recognition.load_image_file("user2.JPG")
user_2_face_encoding = face_recognition.face_encodings(user_2_image)[0]

user_3_image = face_recognition.load_image_file("user3.JPG") 
user_3_face_encoding = face_recognition.face_encodings(user_3_image)[0]

user_4_image = face_recognition.load_image_file("user4.JPG") 
user_4_face_encoding = face_recognition.face_encodings(user_4_image)[0]

user_5_image = face_recognition.load_image_file("user5.JPG") 
user_5_face_encoding = face_recognition.face_encodings(user_5_image)[0]

user_6_image = face_recognition.load_image_file("user6.JPG") 
user_6_face_encoding = face_recognition.face_encodings(user_6_image)[0]

user_7_image = face_recognition.load_image_file("user7.JPG") 
user_7_face_encoding = face_recognition.face_encodings(user_7_image)[0]

user_8_image = face_recognition.load_image_file("user8.JPG") 
user_8_face_encoding = face_recognition.face_encodings(user_8_image)[0]

#user_9_image = face_recognition.load_image_file("user9.JPG") 
#user_9_face_encoding = face_recognition.face_encodings(user_9_image)[0]

#user_10_image = face_recognition.load_image_file("user10.JPG")
#user_10_face_encoding = face_recognition.face_encodings(user_10_image)[0]


known_face_encodings = [ 
user_1_face_encoding, 
user_2_face_encoding, 
user_3_face_encoding, 
user_4_face_encoding, 
user_5_face_encoding,
user_6_face_encoding, 
user_7_face_encoding, 
user_8_face_encoding, 
#user_9_face_encoding, 
#user_10_face_encoding
]


known_face_names = [ 
"user 1",
"user 2",
"user 3",
"user 4",
"user 5",
"user 6",
"user 7",
"user 8",
#"user 9",
#"user 10"
]


# Initialize some variables 
face_locations = [] 
face_encodings = [] 
face_names = [] 
process_this_frame = True

while True:
  if tombol.is_pressed: 
    keamanan = 1
    ret, frame = video_capture.read()
    # Resize frame of video to 1/4 size for faster face recognition processing small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)


    # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses) 
    rgb_small_frame = small_frame[:, :, ::-1]


    # Only process every other frame of video to save time if process_this_frame:
       # Find all the faces and face encodings in the current frame of video
       face_locations = face_recognition.face_locations(rgb_small_frame) 
       face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)


        face_names = []
        for face_encoding in face_encodings:
        # Find all the faces and face encodings in the current frame of video
          face_locations = face_recognition.face_locations(rgb_small_frame) 
          matches = face_recognition.compare_faces(known_face_encodings, face_encoding, tolerance=0.45)
          name = "Unknown"


          # # If a match was found in known_face_encodings, just use the first one.
          #if True in matches:
            # first_match_index = matches.index(True)
            # name = known_face_names[first_match_index]


          # Or instead, use the known face with the smallest distance to the new face
          face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)
          best_match_index = np.argmin(face_distances)


          if matches[best_match_index]:
            name = known_face_names[best_match_index] print(name)

          if name=='user 1' or name == 'user 2' or name == 'user 3' or name == 'user 4' or name == 'user 5' or name == 'user 6' or name == 'user 7' or name == 'user 8':
            led.on() 
            sleep(2) 
            led.off() 
            keamanan = 0 
            break
            #False 
            #video_capture.release()
          else:
            led.off() face_names.append(name)


    process_this_frame = not process_this_frame


    # Display the results
    for (top, right, bottom, left), name in zip(face_locations, face_names):
      # Scale back up face locations since the frame we detected in was scaled to 1/4 size
      top *= 4
      right *= 4
      bottom *= 4
      left *= 4


    # Draw a box around the face
    cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)


    # Draw a label with a name below the face
    cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)
    font = cv2.FONT_HERSHEY_DUPLEX
    cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)


    # Display the resulting image 
    cv2.imshow('Video', frame)

      break 
    print('kamera hidup')

  else:
    led.off() 
    print('kamera mati')
    cv2.destroyAllWindows() 
    #video_capture.release()

  if sensor.is_pressed & keamanan ==1: 
  #if tombol_dalam.is_pressed:
    alarm.on() 
    sleep(0.2) 
    alarm.off() 
    sleep(0.2)

  #if sensor == True & keamanan == 0: 
  #keamanan = 1
    if tombol_dalam.is_pressed & sensor.is_pressed: 
    i += 1
    ret, frame = video_capture.read() 
    print(i)
    ret, frame = video_capture.read() 
    cv2.imwrite('user'+str(i)+'.JPG', frame) 
    alarm.on()
    sleep(0.2) 
    alarm.off() 
    sleep(2)
  if tombol_dalam.is_pressed: 
    led.on()
    sleep(2) 
    led.off() 
    keamanan = 0

print('kamera mati') 
video_capture.release() 
cv2.destroyAllWindows()

# Release handle to the webcam
